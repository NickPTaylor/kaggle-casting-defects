{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Objective\n\nThe objective of this model is automate the classification of images of castings from a manufacturing process into one of two categories: \n\n* Defective\n* OK\n\n## Setup\n\nThe following libraries are required for the analysis:"},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"import pathlib\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D\nfrom sklearn.metrics import confusion_matrix\n\nimport casting_defects_plots\nimport casting_defects_utils","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"The following parameters are used to configure the model.  "},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"BATCH_SIZE = 128\nNO_EPOCHS = 5 \nTRAIN_PROP = 0.05","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The configuration settings control the following:\n\n* `BATCH_SIZE`: The NN below is trained using [mini-batch gradient decent](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size).   This parameter sets the size of the mini-batch.\n* `NO_EPOCHS`: An *epoch* refers to the model status after one entire cycle of the dataset has been used to update the weights.  The `NO_EPOCHS` parameter sets the number of complete cycles through the entire data set that the NN will be exposed to.\n* `TRAIN_PROP`: Sets the proportion of data in the training set that will be used.  It should usually be 1.  However, during model development, it can be useful to adjust it to a small fraction e.g. 0.05 to run a quick 'test' of the setup.\n\n## Data\n\nThe `ImageDataGenrator` class from `keras` is used to pre-process raw image data.  A large number of operations are available for [image augmentation](https://machinelearningmastery.com/image-augmentation-deep-learning-keras/).  In this present case, the only pre-processing that is performed is to normalise the images.  Additionally, a validation set is partitioned from the training data."},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"# Paths to data.\ninput_dir = pathlib.Path.cwd().parent  / 'input'\ndata_sets = ('train', 'test')\ntrain_dir, test_dir = [next(input_dir.rglob(ds)) for ds in data_sets]\n\n# Generators.\n# noinspection SpellCheckingInspection\ntrainval_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    validation_split=0.2\n)\n# noinspection SpellCheckingInspection\ntest_datagen = ImageDataGenerator(\n    rescale=1. / 255\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that there are three data sets:\n\n* **Training Set**. Used to compute the updates to the NN weights i.e. for training.\n* **Validation Set**.  Held-out from the training process and used to compute an unbiased loss function score at the completion of each epoch.  This is used to keep track of how well training is progressing.\n* **Test Set**.  Held-out from the training process all together.  The test set is used after all training has taken place to make an unbiased assessment of the final model.\n\nSome examples of training images are shown below."},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"SEED_VALUE = 123456\nexample_images = casting_defects_utils.generate_examples(\n    no_examples=5,\n    idg=trainval_datagen,\n    directory=train_dir,\n    target_size=(300, 300),\n    color_mode='grayscale',\n    seed=SEED_VALUE\n)\ncasting_defects_plots.plot_examples(example_images, figsize=(8, 4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code sets up objects for generating batches of images to feed to the NN."},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"# Setup generators for train, validation and test data sets.\ntrain_generator = trainval_datagen.flow_from_directory(\n    train_dir,\n    target_size=(300, 300),\n    color_mode='grayscale',\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training',\n    seed=SEED_VALUE\n)\n\nvalidation_generator = trainval_datagen.flow_from_directory(\n    train_dir,\n    target_size=(300, 300),\n    color_mode='grayscale',\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation',\n    seed=SEED_VALUE\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(300, 300),\n    color_mode='grayscale',\n    batch_size=1,\n    class_mode='binary',\n    shuffle=False\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The generators above report the total number of images upon invocation.  Below, the total number of classes in the training set are summarised."},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"cnt = dict(casting_defects_utils.summarise_classes(train_generator))\ncnt = {k : (v, 100 * v / sum(cnt.values())) for k, v in cnt.items()}\nmsg = \"{:9s}: {} observations ({:4.1f}%)\"\nfor cls, (count, prop) in cnt.items():\n    print(msg.format(cls, count, prop))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is observed that the training example are fairly well balanced in terms of their class."},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture"},{"metadata":{},"cell_type":"markdown","source":"The model architecture is defined using the `Sequential` API in `keras`.  The architecture consists of 3 alternating `Conv2D` and `MaxPool2D` layers.  The convolutional layers respond to features in the images.  Examples of features may be may be horizontal lines, vertical lines, circles, etc.  In deep learning networks, the complexity of the features detected increases with the layer depth in the network.\n\nThe convolutional layers contain 32, 64 and 32 filters respectively.  During training, the weights will be updated such that each of the filters will respond to different features in the training set.  The training processes involves manipulating the NN weights such that the identified combination and layout of features (e.g. dark/light spots, jagged/smooth lines, variance/invariance) in a particular image will *activate* the most likely class to which the image belongs.\n\nThe pooling layers aggregate the outputs from the convolutions which has the advantage of a) reducing the number of weights required to train the network and b) combating over-fitting.\n\nThe final Flatten and Dense layers connect the output from the preceding layer to a single node.  This problem is one of *binary classification*.  The *sigmoid* activation function on the final node will vary between 0 and 1 and will model the probability of the image at the input to the NN corresponding to one class (1) or the other (0)."},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"# Create neural network architecture.\nmodel = Sequential([\n    Conv2D(32, kernel_size=3, input_shape=(300, 300, 1), activation='relu'),\n    MaxPool2D(4),\n    Conv2D(64, kernel_size=3, input_shape=(300, 300, 1), activation='relu'),\n    MaxPool2D(4),\n    Conv2D(32, kernel_size=3, input_shape=(300, 300, 1), activation='relu'),\n    Flatten(),\n    Dense(1, activation='sigmoid')\n])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model summary shows the number of parameters (weights).  It is useful to consider this in the context of the size of the training set i.e. in general, the more parameters that there are, the more training data required.\n\n## Model Compilation\n\nBefore the model can be trained, it must be *compiled*.  Firstly, an optimiser is specified.  In this case, the **adam** optimiser is chosen which, in summary, is [gradient decent](https://machinelearningmastery.com/gradient-descent-for-machine-learning/) featuring a learning rate that adjusts as training progresses. The name 'adam' is derived from 'Adaptive Moment Estimation' and it is often recommended as a robust optimisation algorithm.\n\nThe loss function is set to 'binary cross-entropy' which is appropriate for the current class of problem i.e. binary categorisation.  The binary cross-entropy quantifies how correct the model is.  Additionally, the metric, 'accuracy' is specified.  This is not used to assess the model but it is used in the output since it is easier to interpret than binary cross entropy."},{"metadata":{"pycharm":{"name":"#%%  \n","is_executing":false},"trusted":false},"cell_type":"code","source":"model.compile(optimizer='adam', loss='binary_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training \n\nThe model is now ready to be trained.  The train steps per epoch are set such that most images in the training set will be used for training in each epoch and each image will only be used once in an epoch.  The aim is to expose to the NN to every image once in each epoch but since the number of training examples may not be exactly divisible by the `BATCH_SIZE`, some images may not be included in a given epoch.\n\nAfter training the model, *learning curves* are plotted.  They show the history of the loss function computed for the training and validation data at the end of each epoch.  The learning curves are useful for assessing the quality of the model.   Also, they are a useful tool for diagnosing issues such as over-fitting or under-fitting, whether the model might benefit from running more epochs or whether it may benefit from more data.  A 'good' training curve would show both the training and validation error converging, in a stable fashion, to a small and similar loss function value."},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"# Train model.\ntrain_steps, val_steps = [TRAIN_PROP * (gen.samples // BATCH_SIZE)\n                          for gen in (train_generator, validation_generator)]\ntraining = model.fit_generator(\n    generator=train_generator,\n    steps_per_epoch=train_steps,\n    validation_data=validation_generator,\n    validation_steps=val_steps,\n    epochs=NO_EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate model \n\nThe final model is evaluated on the *test* set.  The accuracy is as follows:"},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"test_loss, test_acc = model.evaluate_generator(test_generator)\nprint(\"Accuracy on test set: {:.3f}\".format(test_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Often, [confusion matrices](https://machinelearningmastery.com/confusion-matrix-machine-learning/) are a more appropriate method for assessing performance, especially for unbalanced classes.  The confusion matrix is as follows:"},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":false},"trusted":false},"cell_type":"code","source":"# Calculate predicted probabilities, compute confusion matrix, plot.\npred_prob = model.predict_generator(test_generator)\npred_class = [1 if p > 0.5 else 0 for p in pred_prob]\ntrue_class = test_generator.classes\nlabels = {v: k for k, v in test_generator.class_indices.items()}\nlabels = [labels[0], labels[1]]\n\ncm = confusion_matrix(pred_class, true_class)\n\ncasting_defects_plots.plot_confusion_matrix(cm, labels=labels)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}}},"nbformat":4,"nbformat_minor":4}